Discussion
==========

In this section, we first give an intuitive interpretation of how the Global Connectivity Estimation algorithm works. Then, we present our OpenCL solution: *oclptx*. Finally, we show the results of *oclptx*, comparing both the correctness of the paths as well as the speed to the old tracking software.

Algorithmic Theory
------------------

The tracking algorithm fundamentally executes the following routine:

#. Load multiple sets of fiber parameters (:math:`F, \phi, \theta`) created by the bedpostx step. These parameters are the anisotropy and vector field parameters, respectively.
#. Repeatedly drop imaginary “particles” into the space of the brain. In this step, the algorithm treats the estimated fiber directions as flow directions.
#. Step through the brain via the flow directions, a fixed distance at a time, until some stopping condition is reached. The stopping condition is typically a user-defined mask.

The starting location of particles is determined by user-defined seeds. Seeds can be defined individually in text-format, or as a series of volumes and surfaces which the program automatically iterates through. For each seed, many particles are instantiated. Each particle begins at a random location within a sphere of its seed point. Paths are then interpolated from this starting location, using the fiber distribution. Since the particle does not have an initial direction to help it decide which way to follow the first non-directional fibre, typically it will be run twice---once in each direction. The relation between seeds, particles, and paths is shown in Figure 4.1.

.. _particle:

.. figure:: images/seed_part_path.pdf

  Relation between seeds, particles, and paths.

As the particle is traveling through voxels, the adjacent voxel from which we sample the next direction is randomly selected.  The selected direction is inversely proportional to the distance from the nearest vertex. This process is shown in Figure 3.2.  Once we know the next vertex, a fiber orientation must be determined.  There are several options for this, all of which are selected by the user at start time.  Either a preferred direction is chosen, or the sampler chooses one of the following options: random with error weights, random above a given error threshold, or entirely random.

.. _pathing:

.. figure:: images/pathing.pdf

  Tracking and Pathing

Methods and Architecture
------------------------

*oclptx* fits as a separate module in the FSL suite (Figure 3.3). The code takes identical command-line parameters from a user using *oclptx* as from one using *probtrackx*. These parameters are primarily bedpostx data, along with masks and other user-specified options.

.. _fsl:

.. figure:: images/fsl.png

  Simplified diagram of FSL: bedpostx results are parsed and converted for use on the GPU via a "*samplemanager*" class.

Within *oclptx*, we architect our code around three steps in the pipeline: Initialization, Tracking, and Reduction (Figure 3.4).

..
  .. _oclptx:

  Block diagram of the *oclptx* pipeline. The :math:`F, \phi, \theta` parameters are read in via the initialization step, and then passed into the GPU interpolation and reduction scheme. TODO Steve: FIX

Initialization
^^^^^^^^^^^^^^

In the initialization stage, we manage our data in a custom class called *samplemanager*. The purpose of *samplemanager* is to store static bedpostx/mask data, and provide easy access to the data for the tracking routine. We implemented this class to prevent the duplication of data containers that were used inefficiently in *probtrackx*. *samplemanager* interacts directly with the old *probtrackx* command-line-parser, reading in the myriad of data and options that the user can input (for a complete list of the options, see the Appendix).  The time taken to initialize data does not factor into the speed computations presented in our results.

On the GPU, *oclptx* deals with two types of data: *static data* and *dynamic particle data*. Static data encapsulates any large structures that are written to the GPU once, and which stay in VRAM until all computations are finished. These include bedpostx diffusion parameters as well as any tracking masks. The *oclenv* class is responsible for managing static data once it has been loaded onto the GPU. The following datasets are allocated on buffers into *each* available GPU device during initialization:

* **bedpostx parameter data**: :math:`F, \phi, \theta`, computed and stored in files for *oclptx* to load.

TODO Steve: Define these please (here, not in glossary)

* **Brain mask**.

* **Exclusion mask** (optional).

* **Termination mask** (optional).

* **Waypoint mask** (optional).

* **Global PDF**.

The largest dataset to allocate on the GPU is the parameter data. Parameter data may come in a variety of sizes with respect to the voxel grid. *oclptx* supports any dataset that can fit in the vendor-suggested maximum memory allocation parameter, specific to particular GPU device. For example, sample data obtained from CFRI has dimensions of 256x271x102. Given 32-bit float values, one principal direction of one type of parameter with an average 50 samples per parameter uses ~920MB of VRAM. For consumer level GPU's, the maximum memory allocation parameter is typically ~1/8 of the total VRAM. Therefore, for the CFRI sample data, only high end workstation cards with 6GB+ of VRAM will be able to allocate this memory. It is up to the user to ensure that their data will fit on the device.

  .. _oclptxhandler:
      
    Make this a block diagram of *oclenv*. TODO STEVE: REDO THIS

After static data containers are allocated and synchronized by the *oclenv* class, the remaining GPU memory is determined in bytes, and that value is sent to *oclptxhandler*. *oclptxhandler* uses user-specified options to determine how much memory needs to be allocated per particle, and subsequently maximizes the number of particles that will fit in the remaining memory. The following containers are allocated *per particle*:

* **State**: a struct describing the *persistent* state of a particle:  its current position, last step direction, and the state of its RNG.
* **PDF mask**: a mask of the entire brain volume.  The particle traces its path through this space.  These values are later added to the global sum.
* **Path**: an array for the particle to log its full path.  This is used only for debugging and is disabled in normal operation.
* **Waypoints**: a list indicating which waypoints a particle has encountered.  This is only included if the waypoint masks is enabled.
* **Loopcheck**: a flow field indicating both where we've been and what direction we were heading when we were there.  It it used to determine whether we are currently retracing our steps, and is optional.

To mirror the "dual direction" functionality in *probtrackx* (every particle is instantiated twice, and starts tracking in opposite directions), the starting step direction of each particle is instantiated as {+/-1., 0., 0.}. The specifics of how the containers above are used in tracking is discussed in the next section.

Each particle also has its own RNG.  This ensures that the actual path that is followed is independant of where the particle is processed and the order of processing.  The end result is a fully deterministic system.  The makes results easily replicated across diverse hardware setups, which is a primary design goal, and crucial part of the scientific method.

Tracking
^^^^^^^^

When a batch of particles is sent to the GPU(s) for tracking, two distinct kernels operate on that batch: **interpolation** and **summing**. Every operation in the interpolation GPU kernel mirrors functionality from the Streamline::Streamlines method in *probtrackx*. The interpolation kernel's work range is simply the space of queued particles. To reduce the need for atomic operations, the summing kernel's work range is actually the global voxel space separated into 32 voxel regions. Each summing thread obtains data from all valid particles, and performs summing on its particular section.

  .. _interpolation_flow:

      Basic overview of tracking algorithm kernel. TODO Steve: Add

Any optional tracking steps in the kernels are not compiled if they are unused. This is to reduce any penalties from repeatedly calling conditional statements in the kernel, which decrease performance as the particle count increases.

Interpolation
.............

Interpolation on the GPU is done on a per-particle basis. Each thread processes the stepping of a single particle. Upon startup, the last position, RNG seed value, and unit jump direction are loaded.  Then, probabilistic tracking begins.  The RNG contains independant state for each particle.  The RNG implemented returns 64 bits of randomness per call, and contain 5, 64 bit words of state per RNG.  It has a period of :math:`2^258`, as described in [lecuyer99].  The RNG can be optionally disabled, for testing.  If a particle terminates tracking for any reason, it is assigned an exit code.  The code is used by the summing kernel and reduction algorithm to determine whether or not the particle should be included in the output.

.. [lecuyer99]  P. L'Ecuyer, "Tables of Maximally Equidistributed Combined LFSR Generators", Mathematics of Computation, 68, 225 (1999), 261--269.  http://www.ams.org/journals/mcom/1999-68-225/S0025-5718-99-01039-X/S0025-5718-99-01039-X.pdf

When a particle is loaded into VRAM and the maximum number of steps per kernel are specified, probabilistic tracking proceeds in the following manner:

#.  **Corner Selection** - The particle is surrounded by a diffusion field.  Since it is always between voxels, it uses the local RNG to decide which voxel it will use.  It picks the nearest voxel with the highest likelihood, and further ones with lower likelihood.
#.  **Sample Selection** - One of the many samples at this position is selected at random.
#.  **Stepping** - Use Euler's method to compute a new position based on the selected diffusion sample.  Since the diffusion samples are *directionless*, we choose the direction that minimizes curvature.  Alternately, the user can specify to use the *backward Euler method* instead, which uses the diffusion direction at the target, instead.
#.  **Mask Checking** - Various mask conditions (conditions on our current position) are checked.  These are position-based reasons to potentially stop tracking.

    #. Brain Mask - the outer limit of the brain.
    #. Exclusion Mask - a user specified region to ignore all particles that enter.
    #. Termination Mask - a user specified region to include all particles that enter.
    #. Waypoint Mask(s) - a user specified list of 'waypoints' (think of a rally race)
    #. Curvature Threshold - an upper limit on tightness of a curve.  Particles that curve tighter will be stopped.

#.  **Updating Output Data** - If the temporary position passes all of the specified checks, it is considered valid. The particle's position and last jump direction can be updated in two ways:

    #. **Local connectivity mask update** - If the temporary position is accepted, its position in the voxel space is determined by an integer mapping.  This position in the (particle specific) connectivity mask is set to **1**.
    #. **Particle Path Update** - If the whole path is to be saved, the particle's new position is written to a buffer, which can be dumped to file later.

#.  **Path Length Check** - Check whether this path has reached the maximum path length.  If so, stop.

The runtime of the interpolation kernel depends solely on the configured number of steps per reduce-interpolate cycle.

Summing
.......

Once interpolation *finishes*, the summing kernel is queued immediately. Each thread of the summing kernel operates on a section of the voxel space. In each thread, every particle is checked for the appropriate termination conditions. If the conditions are satisfied, that particle's 32-bit entry pdf is added to the global pdf. The following operations are performed for each particle:

#.  **Check Inclusion Conditions** - As mentioned above, a particle's individual pdf mask may or may not be added to the global total when the summing kernel is executed. There are three possible conditions to check:

    #. **Termination** - If the particle has terminated, the termination code has an integer value > 0. Only these particles are considered for summing.
    #. **Waypoint Condition** - Only valid if the user provides Waypoint masks. For a particle that has finished tracking, we check whether the particle has passed through all of the waypoints. This condition can either be specified as an AND or OR operation: the former requires passage through all masks, and the latter requires passage through only one. Whatever the condition, the particle must meet it to be included in the pdf.
    #. **Exclusion** - If the termination code has an integer value > 0, and is not any of BREAK_INIT, BREAK_INVALID, STILL_FINISHED, then the particle is excluded. In other words, it is not added to the pdf.

#. **Summing** - Each binary mask section of each *included* particle is summed, and the global pdf voxels are incremented appropriately. The individual particle masks are bit masks. As such, for a particular voxel, the aggregate sum is obtained by shifting and summing individual particle bit masks in a local memory container. The result is then added to the global pdf at particle completion.

The summing kernel is expected to take longer to run than the interpolation kernel for large particle sets, as it is O(V) where V is the number of voxels. The complexity value can reach up to tens of millions for large voxel dimensions.

Reduction
^^^^^^^^^

In a high-throughput system like ours, as the system scales up, the task of dispatching data to processors can take longer than the actual data processing.  This effect is well documented in the literature.  Our target machine is a high-performance workstation with 16 modern CPU cores, and 4 high-end graphics cards. As such, it is important to ensure the dispatch system is efficient and scalable.

At start time, we create a global first-in first-out (FIFO) queue of particles.  We load this FIFO with the start position of each particle, as well as the initial state of that particle's RNG.  These are eventually loaded onto the GPU processors.

We start the parallelization at the device level.  For each GPU on the system, we create a *Worker* thread.  Each thread is responsible for copying data to and from a single GPU, and starting processing kernels on that GPU.  Since a single thread handles this work, no synchronization is needed here, making it easier to develop and debug.  Additionally, since *Workers* are independent, we support heterogeneous platforms. These include both low-end and high-end cards, or even cards from multiple vendors. We can support these on the same system without any additional work.

After initializing, each *Worker* creates its own work pool of *Reducers*.  It then loads a batch of processed particles off of the GPU. At this point, the reducers search linearly for finished particles.  Each time a finished particle is found, the Reducer grabs a new particle off of our global FIFO of particles.  It stores these particles in a list, which is passed back to the *Worker*.  The FIFO is the only thing shared between the threads. The whole procedure is outlined in figure :ref:`threading`.

TODO Jeff: Conceptual diagram from first presentations

.. _threading:
.. figure:: images/threading.pdf

    Threading scheme overview flow chart.

Results and Analysis
--------------------

Implementing the aforementioned pipeline, we have arrived at a GPU-based tracking system that can be validated for both correctness as well as speed. The results presented in this section are organized as follows:

* We first show a deterministic comparison between *oclptx* and *probtrackx*, proving validity as well as computation speed increase.
* Next, we show the fully probabilistic result of *oclptx* using our custom PRNG routine.
* Finally, we present our work on multi-threaded reduction.

We recognize that proving convergence on the probabilistic scenario is necessary to completely prove correctness. While we can produce probablistic results, time did not allow for us to set up the statistical t-tests necessary for validation. Our sponsor has offered to complete this task himself, given that a deterministic match is the critical step.

Deterministic Comparison of *oclptx* and *probtrackx*
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

To test the convergence of our results with expected behaviour, we modified the source code of *probtrackx* (ptx2) such that it does not seed randomly. Instead, we forced ptx2 to seed at a predefined location in the brain mask, and we seeded our *oclptx* at that same location in the same data set. Hence, by proving that in this deterministic scenario our interpolation algorithm produces exactly the same set of streamlines as the old code, we prove that our algorithm matches the expected results.

We show these results in :ref:`deterministiccomp`. In this data set, we use 1000 seed points and generated 2000 particles, with a maximum of 2000 steps per path. These parameters give us approximately 4 million points. We then sum over these points to determine the error between the results, :math:`\chi ^ 2`, as follows:

:math:`\chi^2 =  \sum (r^{(1)}_i - r^{(2)}_i)^2  =  O(10^{-11}) \rightarrow  { \langle \chi^2 \rangle } \simeq {O(10^{-13})}`

.. _deterministiccomp:

.. figure:: images/deterministiccomp.pdf

    Deterministic comparison of paths generated by **probtrackx** and *oclptx*, plotted with Python.


Thus, we have a statistcally negligible error. The differences we have can be accounted for if we consider deviations in how the GPU calculates quantities such as sine or cosine. Fundementally, we have shown that the deterministic results of the two pipelines are identical.

Next, we compare the run time of *oclptx* against that of ptx2. For a 4000 particle sample dataset, a deterministic run of ptx2 has an average computation time of **43 seconds** from the point of beginning to interpolate to the point of completion. *oclptx*, for the same dataset, has an average tracking time of **500-1500 milliseconds**. The following table compares the two algorithms for various particle sizes.

.. table:: Computation times of *oclptx* and Ptx2 compared.
   
   ===========  =========  =============
   Sample Size  Ptx2 Time  *oclptx* Time
   ===========  =========  =============
   4000          43s         1000ms
   ===========  =========  =============


TODO: STEVE This is stuff for you; I wrote to the best of my knowledge and research on our results. You can modify/add as needed. Discuss the significance of the results only within the problem space (not for the greater good of humanity or someshit). NO RECOMMENDATIONS HERE

TODO (steve): going to perform some actual timing benchmarking tonight and include 3-4 plots here.

Probabilistic Results of *oclptx*
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

TODO STEVE: Describe the results and show the pictures of the biased tracking. I've already stated in the opening that we don't test against the old PTX2 on this step. Here you can expand on what tests NEED to be done by Danny. NO RECOMMENDATIONS HERE

The RNG was tested against a CPU-based reference implementation.  100,000 values were compared over 500 independant seed values, and matched the reference implementation exactly.  We also histogrammed them for qualititative value.

.. _probibilistic1:
.. figure:: images/probibilistic1.pdf

    Caption here as needed

.. _prng:
.. figure:: images/prng.png

    Caption here as needed

Multi-Threaded Reduction
^^^^^^^^^^^^^^^^^^^^^^^^

The threading scheme was tested independently of the interpolation kernel. Here, we used a placeholder kernel which acts like the interpolation kernel, but instead of implementing probabilistic tractography, we implemented a well-known mathematics problem called the *Collatz Conjecture*[collatzwiki]_.  It was tested both on a small-scale desktop computer with a single CPU as well as CFRI's high-end workstation.  The run time for 1,000,000 particles, starting in the range 1--10,000,000 (TODO JEFF: What is this range?) are depicted in Figures :ref:`collatz_stepsper` and :ref:`collatz_workpool`.  All data was verified against a reference implementation, which gives us confidence that the threading scheme is well-implemented and reliable.

TODO Jeff: Resolve References here. Collatz picture, Step-length vs time, PROFILING TABLE

.. [collatzwiki] Collatz conjecture --- Wikipedia, the free encyclopedia http://en.wikipedia.org/wiki/Collatz_conjecture

