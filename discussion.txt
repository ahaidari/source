Discussion
==========

In this section, we first give an intuitive interpretation of how the Global Connectivity Estimation algorithm works. Then, we present our OpenCL solution: *oclptx*. Finally, we show the results of *oclptx*, comparing both the correctness of the paths as well as the speed to the old tracking software.

Algorithmic Theory
------------------

The tracking algorithm fundamentally executes the following routine:

#. Load multiple sets of fiber parameters (:math:`F, \phi, \theta`) created by the bedpostx step. These parameters are the anisotropy and vector field parameters, respectively.
#. Repeatedly drop imaginary “particles” into the space of the brain. In this step, the algorithm treats the estimated fiber directions as flow directions.
#. Step through the brain via the flow directions, a fixed distance at a time, until some stopping condition is reached. The stopping condition is typically a user-defined mask.

The starting location of particles is determined by user-defined seeds. Seeds can be defined individually in text-format, or as a series of volumes and surfaces which the program automatically iterates through. For each seed, many particles are instantiated. Each particle begins at a random location within a sphere of its seed point. Paths are then interpolated from this starting location, using the fiber distribution. Since the particle does not have an initial direction to help it decide which way to follow the first non-directional fibre, typically it will be run twice---once in each direction. The relation between seeds, particles, and paths is shown in Figure 4.1.

.. _particle:

.. figure:: images/seed_part_path.pdf

  Relation between seeds, particles, and paths.

As the particle is traveling through voxels, the adjacent voxel from which we sample the next direction is randomly selected.  The selected direction is inversely proportional to the distance from the nearest vertex. This process is shown in Figure 4.2.  Once we know the next vertex, a fiber orientation must be determined.  There are several options for this, all of which are selected by the user at start time.  Either a preferred direction is chosen, or the sampler chooses one of the following options: random with error weights, random above a given error threshold, or entirely random.

.. _pathing:

.. figure:: images/pathing.pdf

  Tracking and Pathing

Methods and Architecture
------------------------

*oclptx* fits as a separate module in the FSL suite that depends on several existing I/O classes. The code takes identical command-line parameters from a user using *oclptx* as from one using *probtrackx*. These parameters are primarily bedpostx data, along with masks and other user-specified options.

Within *oclptx*, we architect our code around three steps in the pipeline: Initialization, Tracking, and Reduction (Figure 4.3).

..  _oclptx:

..  figure:: images/oclptx_block_diagram.pdf

    Block diagram of the full *oclptx* pipeline.


Initialization
^^^^^^^^^^^^^^

In the initialization stage, we manage our data in a custom class called *samplemanager*. The purpose of *samplemanager* is to store static bedpostx/mask data, and provide easy access to the data for the tracking routine. We implemented this class to prevent the duplication of data containers that were used inefficiently in *probtrackx*. *samplemanager* interacts directly with the old *probtrackx* command-line-parser, reading in the myriad of data and options that the user can input (for a complete list of the options, see the Appendix).  The time taken to initialize data does not factor into the speed computations presented in our results.

On the GPU, *oclptx* deals with two types of data: *static data* and *dynamic particle data*. Static data encapsulates any large structures that are written to the GPU once, and which stay in VRAM until all computations are finished. These include *bedpostx* diffusion parameters as well as any tracking masks. The *oclenv* class is responsible for managing static data once it has been loaded onto the GPU. The following datasets are allocated on buffers into *each* available GPU device during initialization: bedpostx parameter data (:math:`F, \phi, \theta`), Brain Mask, Exclusion Mask, Termination Mask, Waypoint Mask(s), and the Global Connectivity Map. The Connectivity Map is the output of our program. We further discuss the masks and their meaning in the Tracking section of this report.

The largest dataset to allocate on the GPU is the parameter data. Parameter data may come in a variety of sizes with respect to the voxel grid. *oclptx* supports any dataset that can fit in the vendor-suggested maximum memory allocation parameter, specific to particular GPU device. For example, sample data obtained from CFRI has dimensions of 256x271x102. Given 32-bit float values, one principal direction of one type of parameter with an average 50 samples per parameter uses ~920MB of VRAM. For consumer level GPU's, the maximum memory allocation parameter is typically ~1/8 of the total VRAM. Therefore, for the CFRI sample data, only high end workstation cards with 6GB+ of VRAM will be able to allocate this memory. It is up to the user to ensure that their data will fit on the device.

.. _oclenv:

  ..  figure:: images/oclenv_block_diagram.pdf

      OclEnv functional block diagram.

After static data containers are allocated and synchronized by the *oclenv* class (Figure 4.4), the remaining GPU memory is determined in bytes, and that value is sent to *oclptxhandler*. *oclptxhandler* uses user-specified options to determine how much memory needs to be allocated per particle, and subsequently maximizes the number of particles that will fit in the remaining memory. The following containers are allocated *per particle*:

* **State**: a struct describing the *persistent* state of a particle:  its current position, last step direction, and the state of its RNG.
* **Global Connectivity mask**: a mask of the entire brain volume.  The particle traces its path through this space.  These values are later added to the global sum.
* **Path**: an array for the particle to log its full path.  This is used only for debugging and is disabled in normal operation.
* **Waypoints**: a list indicating which waypoints a particle has encountered.  This is only included if the waypoint masks is enabled.
* **Loopcheck**: a flow field indicating both where the particle has been and what direction it was heading when it was there.  It is an option, used to determine whether we are currently retracing our steps.

Figure 4.5 visualizes the static and dynamic containers used on the GPU.

.. _containers:

  ..  figure:: images/oclptx_data_containers.pdf

      Overview of all data used by olcptx and how memory sizes are calculated.

To mirror the "dual direction" functionality in *probtrackx* (every particle is instantiated twice, and starts tracking in opposite directions), the starting step direction of each particle is instantiated as {+/-1., 0., 0.}. The specifics of how the containers above are used in tracking is discussed in the next section.

Each particle also has its own RNG.  This ensures that the actual path that is followed is independent of where the particle is processed and the order of processing. We further discuss the particle's use of RNG in our discussion of Tracking.

Tracking
^^^^^^^^

When a batch of particles is sent to the GPU(s) for tracking, two distinct kernels operate on that batch: **interpolation** and **summing**. Every operation in the interpolation GPU kernel mirrors functionality from the Streamline::Streamlines method in *probtrackx*. The interpolation kernel's work range is simply the space of queued particles. To reduce the need for atomic operations, the summing kernel's work range is actually the global voxel space separated into 32 voxel regions. Each summing thread obtains data from all valid particles, and performs summing on its particular section.

Any optional tracking steps in the kernels are not compiled if they are unused. This is to reduce any penalties from repeatedly calling conditional statements in the kernel, which decrease performance as the particle count increases.

Interpolation
.............

Interpolation on the GPU is done on a per-particle basis. Each thread processes the stepping of a single particle. Upon startup, the last position, RNG seed value, and unit jump direction are loaded.  Then, probabilistic tracking begins.  The RNG contains independent states for each particle.  It returns 64 bits of randomness per call, and contain five 64-bit words of state per RNG.  It has a period of :math:`2^258`, as described in [lecuyer99]_.  The RNG can be optionally disabled for testing.  If a particle terminates tracking for any reason, it is assigned an exit code.  The code is used by the summing kernel and reduction algorithm to determine whether or not the particle should be included in the output.

.. [lecuyer99]  P. L'Ecuyer, "Tables of Maximally Equidistributed Combined LFSR Generators", Mathematics of Computation, 68, 225 (1999), 261--269.  http://www.ams.org/journals/mcom/1999-68-225/S0025-5718-99-01039-X/S0025-5718-99-01039-X.pdf

When a particle is loaded into VRAM and the maximum number of steps per kernel are specified, probabilistic tracking proceeds in the following manner:

#.  **Corner Selection** - The particle is surrounded by a diffusion field.  Since it is always between voxels, it uses the local RNG to decide which voxel it will use next.  It picks the nearest voxel with the highest likelihood, and further ones with lower likelihood.
#.  **Sample Selection** - One of the many samples at this position is selected at random.
#.  **Stepping** - Euler's method is used to compute a new position based on the selected diffusion sample.  Since the diffusion samples are *directionless*, we choose the direction that minimizes curvature.  Alternately, the user can specify to use the *Improved Euler method*, which uses the diffusion direction at the target.
#.  **Mask Checking** - Various mask conditions (conditions on our current position) are checked.  These are position-based reasons to potentially stop tracking.

    #. **Brain Mask** - the outer limit of the brain.
    #. **Exclusion Mask** - a user specified region to ignore all particles that enter.
    #. **Termination Mask** - a user specified region to terminate tracking immediately but still include all particles that enter.
    #. **Waypoint Mask(s)** - a user specified list of 'waypoints'. If specified, the particle must pass through these all or at least one points, depending on user options, if the path is to be valid.
    #. **Curvature Threshold** - an upper limit on tightness of a curve.  Particles that curve tighter will be stopped.

#.  **Updating Output Data** - If the particle position passes all of the specified checks, it is considered valid. The particle's position and last jump direction can be updated in two ways:

    #. **Local Connectivity Mask update** - If the temporary position is accepted, its position in the voxel space is determined by an integer mapping.  This position in the connectivity mask is set to **1**.
    #. **Particle Path Update** - If the whole path is to be saved, the particle's new position is written to a buffer, which can be dumped to file.

#.  **Path Length Check** - Check whether this path has reached the maximum path length.  If so, stop.

The runtime of the interpolation kernel depends solely on the configured number of steps per reduce-interpolate cycle. When a particle terminates, it is assigned an exit code. This exit code is used by the reduction threads to determine the status of the particle and whether processing will continue.

Summing
.......

Once interpolation *finishes*, the summing kernel is queued immediately. Each thread of the summing kernel operates on a section of the voxel space. In each thread, every particle is checked for the appropriate termination conditions. If the conditions are satisfied, that particle's 32-bit connectivity map is added to the global connectivity map. The following operations are performed for each particle:

#.  **Check Inclusion Conditions** - As mentioned above, a particle's individual connectivity mask may or may not be added to the global total when the summing kernel is executed. There are three possible conditions to check:

    #. **Termination** - If the particle has terminated, the termination code has an integer value > 0. Only these particles are considered for summing.
    #. **Waypoint Condition** - Only valid if the user provides Waypoint masks. For a particle that has finished tracking, we check whether the particle has passed through all of the waypoints. This condition can either be specified as an AND or OR operation: the former requires passage through all masks, and the latter requires passage through only one. Whatever the condition, the particle must meet it to be included in the global connectivity map.
    #. **Exclusion** - If the termination code has an integer value > 0, and is not any of BREAK_INIT, BREAK_INVALID, STILL_FINISHED, then the particle is excluded. In other words, it is not added to the global connectivity map.

#. **Summing** - Each binary mask section of each *included* particle is summed, and the global connectivity voxels are incremented appropriately. The individual particle masks are bit masks. As such, for a particular voxel, the aggregate sum is obtained by shifting and summing individual particle bit masks in a local memory container. The result is then added to the global connectivity map at particle completion.

The summing kernel is expected to take longer to run than the interpolation kernel for large particle sets, as it is O(V) where V is the number of voxels. The complexity value can reach up to tens of millions for large voxel dimensions.

Reduction
^^^^^^^^^

In a high-throughput system like ours, as the system scales up, the task of dispatching data to processors can take longer than the actual data processing.  This effect is well documented in the literature.  Our target machine is a high-performance workstation with 16 modern CPU cores, and 4 high-end graphics cards. As such, it is important to ensure the dispatch system is efficient and scalable.

At start time, we create a global first-in first-out (FIFO) queue of particles.  We load this FIFO with the start position of each particle, as well as the initial state of that particle's RNG.  These are eventually loaded onto the GPU processors.

We start the parallelization at the device level.  For each GPU on the system, we create a *Worker* thread.  Each thread is responsible for copying data to and from a single GPU, and starting processing kernels on that GPU.  Since a single thread handles this work, no synchronization is needed here, making it easier to develop and debug.  Additionally, since *Workers* are independent, we support heterogeneous platforms. These include both low-end and high-end cards, or even cards from multiple vendors. We can support these on the same system without any additional work.

After initializing, each *Worker* creates its own work pool of *Reducers*.  It then loads a batch of processed particles off of the GPU. At this point, the reducers search linearly for finished particles.  Each time a finished particle is found, the Reducer grabs a new particle off of our global FIFO of particles.  It stores these particles in a list, which is passed back to the *Worker*.  The FIFO is the only thing shared between the threads. The whole procedure is outlined in Figure 4.6.

.. _threading:
.. figure:: images/threading.pdf

    Threading scheme overview flow chart.

Results and Analysis
--------------------

Implementing the aforementioned pipeline, we have arrived at a GPU-based tracking system that can be validated for both correctness as well as speed. The results presented in this section are organized as follows:

* We first show a deterministic comparison between *oclptx* and *probtrackx*, proving validity as well as computation speed increase.
* As the sponsor specified a required performance speedup, we run speed tests on varying amounts of particles to compare the completion time of *oclptx* to *ptx2*
* Next, we show the fully probabilistic result of *oclptx* using our custom PRNG routine.
* Finally, we present our work on multi-threaded reduction.

We recognize that proving convergence on the probabilistic scenario is necessary to completely prove correctness. While we can produce probabilistic results, time did not allow for us to set up the statistical t-tests necessary for validation. Our sponsor has offered to complete this task himself, given that the deterministic match is the critical step.

Deterministic Comparison of *oclptx* and *probtrackx*
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

To test the convergence of our results with expected behaviour, we modified the source code of *probtrackx* such that it does not seed randomly. Instead, we forced ptx2 to seed at a predefined location in the brain mask, and we seeded our *oclptx* at that same location in the same data set. Hence, by proving that in this deterministic scenario our interpolation algorithm produces exactly the same set of streamlines as the old code, we prove that our algorithm matches the expected results.

We show these results in Figure 4.7. In this data set, we use 1000 seed points and generated 2000 particles, with a maximum of 2000 steps per path. These parameters give us approximately 4 million points. We then sum over these points to determine the error between the results, :math:`\chi ^ 2`, as follows:

:math:`\chi^2 =  \sum (r^{(1)}_i - r^{(2)}_i)^2  =  O(10^{-11}) \rightarrow  { \langle \chi^2 \rangle } \simeq {O(10^{-13})}`

.. _deterministiccomp:

.. figure:: images/deterministiccomp.pdf

    Deterministic comparison of paths generated by *probtrackx* and *oclptx*, plotted with Python.

Thus, we have a statistcally negligible error. The differences we have can be accounted for if we consider deviations in how the GPU calculates quantities such as sine or cosine. Fundementally, we have shown that the deterministic results of the two pipelines are identical.

Computational Speedup of *oclptx*
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

CFRI has specified a minimum speedup requirement of 18x. This number represents an ideal total completion time for a typical data set on a high-end machine (Table 4.1). For this testing, *oclptx* was forced a reduction set range of 4000 particles and 2000 steps per stage. Seed files with varying amounts of particles were generated, and the total time spent tracking was dumped to a file. Both *oclptx* and *probtrackx* do not count initialization functions in their computation of tracking time.

.. _platform:
.. table:: Benchmarking was done on a personal workstation with the following specifications.

  ===================== ========== =============== ======== ==========
  Processor             Host RAM   GPU (VRAM)      Compiler SDK
  ===================== ========== =============== ======== ==========
  Core i7-950 (4.4GHz)) 12GB  DDR3 Radeo V9800 4GB GCC 4.8  OpenCL 1.2
  ===================== ========== =============== ======== ==========

The tracking time plots show completion times as well as speedup ratios for three different types of options:

* Basic probabilistic tracking, with just a brain mask and curvature threshold for termination conditions (Figure 4.8).
* Basic tracking, with Improved Euler interpolation (Figure 4.9).
* Basic tracking, with Improved Euler interpolation *and* particle loopcheck (Figure 4.10).

As the number of particles grows, the speedup ratio eventually asymptotes to some alogrithmic limit, representing the case where the effects of overhead are completely negated. It is interesting that even trivially low particle counts (1000 particles is well below the typical number used, by 2-3 orders of magnitude [Xu]_) experience a significant speedup with *oclptx*. 

With respect to sponsor requirements, the speedup on the single GPU runs for the 50k particles ranged from 9.5x to 11.5x. Because the sponsor has 4 GPU's, they can expect to achieve a speedup of ~4x of our result. It is reasonable to expect that given these computation times, the sponsor will be able to achieve a speedup of ~**38-46x**. This is more than double the specified requirement. Our expectation is reasonable because all GPU handler threads work independently, with the only joining/bottlenecking occurring on startup. The final synchronization of summing all of the global connectivity maps is a trivial operation with respect to processing time.

.. _oclptx_bench_basic:

.. figure:: images/time_trials_basic.png

    Basic options.

.. _oclptx_bench_euler:

.. figure:: images/time_trials_euler.png

    Basic options + euler

.. _oclptx_bench_euler_loopcheck:

.. figure:: images/time_trials_euler_loopcheck.png

    Basic options + euler + loopcheck.


Probabilistic Results of *oclptx*
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

In order to determine complete validity between *oclptx* and *probtrackx*, a standard T-Test must be performed to compare the resulting global connectivity maps. In this case, we no longer remove RNG. Ideally, we would run *probtrackx* a number of times, and then obtain a mean and variance of all of the outputs. Then, we would evaluate an output from *oclptx* for the same seeds and DTI data. The sponsor has from the outset volunteered to do this testing. Therefore, the results we present in this section will not be compared to *probtrackx*

With respect to random number generation, it is import to verify that the algorithm produces the desired distribution across the seed space. In this case, it is a domain of 64-bit integers. The Tausworthe/LFSR method was used to generate our random numbers. The PRNG was validated separately, with the LFSR scheme being applied to 100 different seed states 100K times each. The resulting histogram across the integer space was uniform, as seen in Figure 4.11.

For a qualitative sense of probabilistic vs. deterministic tracking, we can turn off the use of RNG with a CLI flag and compare the data output. The deterministic output (Figure 4.12) is consistently more conforming to a particular set of paths than the probabilistic output (Figure 4.13).

.. _prng_test:

.. figure:: /images/prng.png

    Tausworth/LFSR scheme of *oclptx* distribution. The effective period of the scheme in use is ~:math:`2^258`.

.. _oclptx_norng:

.. figure:: images/oclptx_100k_norng.png

    *oclptx* output with pure deterministic tracking; 102x102x60 DTI data; seeded around (51, 51, 39) in circle of radius 10; 100K particles.

.. _oclptx_rng:

.. figure:: images/oclptx_500k_102_102_60.png

    *oclptx* output with probabilistic tracking; 102x102x60 DTI data; seeded around (51, 51, 39) in circle of radius 10; 100K particles.

Reduction
^^^^^^^^^

The threading scheme was tested independently of the interpolation kernel. Here, we used a placeholder kernel which acts like the interpolation kernel. However, instead of implementing probabilistic tractography, we implemented a well-known mathematics problem called the *Collatz Conjecture* [collatzwiki]_, since it accurately mimics asynchronous threading behaviour.  We tested both on a small-scale desktop computer with a single CPU, as well as CFRI's high-end workstation.  The runtime for 100,000 particles are depicted in Figures 4.14 and 4.15.  All data was verified against a reference implementation, which gives us confidence that the threading scheme is well-implemented and reliable.

.. _collatz_stepsper:
.. figure:: images/collatz_stepsper.pdf

  Runtime of Collatz and Reduction, varying number of steps per reduce-process cycle.  100,000 particles, 2 'Reduce' threads.

The data shown in Figure 4.14 demonstrates how performance fares when we modify the number of steps each interpolation kernel takes before reduction. The last datapoint in the figure shows a dramatic increase in runtime for the step lengths greater than :math:`10^4`. At this point, most of the threads have finished and are idling while a final few threads finish their computations. Mo Xu et al. performed a similar test scenario, and it is useful for optimizing the ideal maximum number of steps parameter in the code.

To understand resource usage on the worst-case machine, we profiled our code on the small-scale single GPU, single CPU desktop. The profiling test demonstrates where time is spent in various parts of the program's lifecycle.  These results are summarized in Table 4.2.


.. _perf_percent:
.. table:: Time spent in parts of our algorithm, as a percentage of total time.

  ================== ===================
  Component          Percent Time [%]
  ================== ===================
  Interpolation      50.9%
  Summing            43.2%
  Write New Data     5.8%
  Read Status        0.1%
  Reduction          <0.01%
  ================== ===================

Table 4.2 presents an interesting point of discussion. In our architecture, Interpolation and Summing are performed on the GPU. Reduction is performed on the CPU, and Write New Data/Read Status are communication between the CPU and GPU. Therefore, on our low-end machine, the program spends 94.1% of its time on the GPU kernels. Mo-Xu et al. present a different result. In their profiling, they obtained ~55% of total time spent on the GPU kernels [Xu]_. There are three reasons for this difference:

#. Mo-Xu et al. were working with a simplified kernel, not requiring the summing step that our sponsor requires.
#. Our GPU code is not as simplistic as Mo Xu's. Our project sponsor asked for focus on the additional features, such as masks, and outputing valid niftii data, once we had obtained a reasonable speedup.
#. We have not had a significant amount of time to optimize, due to ongoing debugging issues on the CFRI workstation. For example, our code still has synchronization blocks in various sections of code that may not be necessary if the pipeline is validated more thoroughly.

As such, though we obtained the sponsor's speedup requirements, our code performance can be improved.  We present our recommendations to improve our code in the recommendations section of this report.

.. [collatzwiki] Collatz conjecture --- Wikipedia, the free encyclopedia http://en.wikipedia.org/wiki/Collatz_conjecture


.. _collatz_workpool:
.. figure:: images/collatz_workpool.pdf

  Runtime of Collatz and Reduction, varying number of 'Reduce' threads.  100,000 particles, 10,000 steps per reduce-process cycle.
